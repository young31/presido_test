{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f35faef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from typing import List, Optional\n",
    "\n",
    "import spacy_transformers\n",
    "from faker import Faker\n",
    "from presidio_analyzer import (\n",
    "    AnalyzerEngine,\n",
    "    EntityRecognizer,\n",
    "    RecognizerResult,\n",
    "    PatternRecognizer,\n",
    "    Pattern,\n",
    ")\n",
    "from presidio_analyzer.nlp_engine import (\n",
    "    NlpEngine,\n",
    "    SpacyNlpEngine,\n",
    "    TransformersNlpEngine,\n",
    "    NerModelConfiguration,\n",
    ")\n",
    "from presidio_analyzer.recognizer_registry import RecognizerRegistry\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "from presidio_anonymizer.entities import OperatorConfig\n",
    "from presidio_analyzer.context_aware_enhancers import LemmaContextAwareEnhancer\n",
    "\n",
    "# --- 1. Enhanced Custom Korean PII Recognizers ---\n",
    "\n",
    "# Regex patterns are improved for better matching.\n",
    "# Handles optional hyphens in phone numbers and more bank account formats.\n",
    "KOREAN_PHONE_NUMBER_REGEX = r\"\\b(?:010|02|0[3-9][0-9])[ -]?(\\d{3,4})[ -]?(\\d{4})\\b\"\n",
    "KOREAN_BANK_ACCOUNT_REGEX = r\"\\b(?:\\d{2,6}[-]?\\d{2,6}[-]?\\d{3,6}|\\d{10,14})\\b\"\n",
    "\n",
    "# Context words improve detection accuracy by boosting the score of a match\n",
    "# when these words are found near the potential PII.\n",
    "CONTEXT_WORDS = {\n",
    "    \"주민등록번호\",\n",
    "    \"주민번호\",\n",
    "    \"주민증\",\n",
    "    \"RRN\",\n",
    "    \"연락처\",\n",
    "    \"휴대폰\",\n",
    "    \"전화번호\",\n",
    "    \"핸드폰\",\n",
    "    \"phone\",\n",
    "    \"mobile\",\n",
    "    \"계좌번호\",\n",
    "    \"계좌\",\n",
    "    \"은행\",\n",
    "    \"bank\",\n",
    "    \"account\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9e7344d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Iterable, Iterator, List, Literal, Optional\n",
    "import warnings\n",
    "\n",
    "from thinc.api import get_torch_default_device\n",
    "from spacy.language import Language\n",
    "from spacy.pipeline import Pipe\n",
    "from spacy.tokens import Doc, Span, SpanGroup\n",
    "from spacy import util\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "@Language.factory(\n",
    "    \"hf_token_pipe\",\n",
    "    assigns=[],\n",
    "    default_config={\n",
    "        \"model\": \"\",\n",
    "        \"revision\": \"main\",\n",
    "        \"stride\": 16,\n",
    "        \"aggregation_strategy\": \"average\",\n",
    "        \"annotate\": \"ents\",\n",
    "        \"annotate_spans_key\": None,\n",
    "        \"alignment_mode\": \"strict\",\n",
    "        \"scorer\": None,\n",
    "        \"kwargs\": {},\n",
    "    },\n",
    "    default_score_weights={},\n",
    ")\n",
    "def make_hf_token_pipe(\n",
    "    nlp: Language,\n",
    "    name: str,\n",
    "    model: str,\n",
    "    revision: str,\n",
    "    # note that the tokenizer stride is the size of the overlap, not the size of\n",
    "    # the stride\n",
    "    stride: Optional[int],\n",
    "    # this is intentionally omitting \"none\" from the aggregation strategies\n",
    "    aggregation_strategy: Literal[\"simple\", \"first\", \"average\", \"max\"],\n",
    "    annotate: Literal[\"ents\", \"pos\", \"spans\", \"tag\"],\n",
    "    annotate_spans_key: Optional[str],\n",
    "    alignment_mode: Literal[\"strict\", \"contract\", \"expand\"],\n",
    "    scorer: Optional[Callable],\n",
    "    kwargs: dict,\n",
    "):\n",
    "    try:\n",
    "        device = get_torch_default_device().index\n",
    "        if device is None:\n",
    "            device = -1\n",
    "    except Exception:\n",
    "        device = -1\n",
    "    if model == \"\":\n",
    "        raise ValueError(\n",
    "            \"No model provided. Specify the model in your config, e.g.:\\n\\n\"\n",
    "            'nlp.add_pipe(\"hf_token_pipe\", config={\"model\": \"dslim/bert-base-NER\"})'\n",
    "        )\n",
    "    hf_pipeline = pipeline(\n",
    "        task=\"token-classification\",\n",
    "        model=model,\n",
    "        revision=revision,\n",
    "        aggregation_strategy=aggregation_strategy,\n",
    "        device=device,\n",
    "        stride=stride,\n",
    "        **kwargs,\n",
    "    )\n",
    "    return HFTokenPipe(\n",
    "        name=name,\n",
    "        hf_pipeline=hf_pipeline,\n",
    "        annotate=annotate,\n",
    "        annotate_spans_key=annotate_spans_key,\n",
    "        alignment_mode=alignment_mode,\n",
    "        scorer=scorer,\n",
    "    )\n",
    "\n",
    "\n",
    "class HFTokenPipe(Pipe):\n",
    "    def __init__(\n",
    "        self,\n",
    "        name: str,\n",
    "        hf_pipeline: pipeline,\n",
    "        *,\n",
    "        annotate: Literal[\"ents\", \"pos\", \"spans\", \"tag\"] = \"ents\",\n",
    "        annotate_spans_key: Optional[str] = None,\n",
    "        alignment_mode: str = \"strict\",\n",
    "        scorer: Optional[Callable] = None,\n",
    "    ):\n",
    "        self.name = name\n",
    "        self.hf_pipeline = hf_pipeline\n",
    "        self.annotate = annotate\n",
    "        if self.annotate == \"spans\":\n",
    "            if isinstance(annotate_spans_key, str):\n",
    "                self.annotate_spans_key = annotate_spans_key\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"'annotate_spans_key' setting required to set spans annotations for hf_token_pipe\"\n",
    "                )\n",
    "        self.alignment_mode = alignment_mode\n",
    "        self.scorer = scorer\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        return next(self.pipe([doc]))\n",
    "\n",
    "    def pipe(self, stream: Iterable[Doc], *, batch_size: int = 128) -> Iterator[Doc]:\n",
    "        for docs in util.minibatch(stream, size=batch_size):\n",
    "            outputs = self._get_annotations(docs)\n",
    "            for doc, output in zip(docs, outputs):\n",
    "                output_spans = SpanGroup(doc, attrs={\"scores\": []})\n",
    "                prev_ann_end = 0\n",
    "                for ann in output:\n",
    "                    if ann[\"start\"] >= prev_ann_end:\n",
    "                        output_span = doc.char_span(\n",
    "                            ann[\"start\"],\n",
    "                            ann[\"end\"],\n",
    "                            label=ann[\"entity_group\"],\n",
    "                            alignment_mode=self.alignment_mode,\n",
    "                        )\n",
    "                        if output_span is not None and output_span.start_char >= prev_ann_end:\n",
    "                            output_spans.append(output_span)\n",
    "                            output_spans.attrs[\"scores\"].append(ann[\"score\"])\n",
    "                            prev_ann_end = ann[\"end\"]\n",
    "                        else:\n",
    "                            text_excerpt = (\n",
    "                                doc.text if len(doc.text) < 100 else doc.text[:100] + \"...\"\n",
    "                            )\n",
    "                            warnings.warn(\n",
    "                                f\"Skipping annotation, {ann} is overlapping or can't be aligned for doc '{text_excerpt}'\"\n",
    "                            )\n",
    "                self._set_annotation_from_spans(doc, output_spans)\n",
    "                yield doc\n",
    "\n",
    "    def _get_annotations(self, docs: List[Doc]) -> List[List[dict]]:\n",
    "        with warnings.catch_warnings():\n",
    "            # the PipelineChunkIterator does not report its length correctly,\n",
    "            # leading to many spurious warnings from torch\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\", message=\"Length of IterableDataset\", category=UserWarning\n",
    "            )\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"You seem to be using the pipelines sequentially on GPU\",\n",
    "                category=UserWarning,\n",
    "            )\n",
    "            if len(docs) > 1:\n",
    "                try:\n",
    "                    return self.hf_pipeline([doc.text for doc in docs])\n",
    "                except Exception:\n",
    "                    warnings.warn(\n",
    "                        \"Unable to process texts as batch, backing off to processing texts individually\"\n",
    "                    )\n",
    "            outputs = []\n",
    "            for doc in docs:\n",
    "                try:\n",
    "                    outputs.append(self.hf_pipeline(doc.text))\n",
    "                except Exception:\n",
    "                    text_excerpt = doc.text if len(doc.text) < 100 else doc.text[:100] + \"...\"\n",
    "                    warnings.warn(\n",
    "                        f\"Unable to process, skipping annotation for doc '{text_excerpt}'\"\n",
    "                    )\n",
    "                    outputs.append([])\n",
    "            return outputs\n",
    "\n",
    "    def _set_annotation_from_spans(self, doc: Doc, spans: SpanGroup) -> Doc:\n",
    "        if self.annotate == \"ents\":\n",
    "            doc.set_ents(list(spans))\n",
    "        elif self.annotate == \"spans\":\n",
    "            doc.spans[self.annotate_spans_key] = spans\n",
    "        elif self.annotate == \"tag\":\n",
    "            for span in spans:\n",
    "                for token in span:\n",
    "                    token.tag_ = span.label_\n",
    "        elif self.annotate == \"pos\":\n",
    "            for span in spans:\n",
    "                for token in span:\n",
    "                    token.pos_ = span.label_\n",
    "        return doc\n",
    "\n",
    "    # dummy serialization methods\n",
    "    def to_bytes(self, **kwargs):\n",
    "        return b\"\"\n",
    "\n",
    "    def from_bytes(self, _bytes_data, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def to_disk(self, _path, **kwargs):\n",
    "        return None\n",
    "\n",
    "    def from_disk(self, _path, **kwargs):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "095ab1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanRRNRecognizer(EntityRecognizer):\n",
    "    ENTITIES = [\"KR_RESIDENT_REGISTRATION_NUMBER\"]\n",
    "    SUPPORTED_LANGUAGE = \"ko\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        self.pattern = Pattern(name=\"rrn_candidate\", regex=r\"(\\d{6})[-]\\d{7}\", score=0.5)\n",
    "        super().__init__(supported_entities=self.ENTITIES, name=\"Korean RRN Recognizer\", **kwargs)\n",
    "\n",
    "    def load(self) -> None:\n",
    "        pass\n",
    "\n",
    "    def analyze(self, text: str, entities: List[str], nlp_artifacts) -> List[RecognizerResult]:\n",
    "        results = []\n",
    "        for match in re.finditer(self.pattern.regex, text):\n",
    "            if self.is_valid_rrn(match.group(0)):\n",
    "                results.append(\n",
    "                    RecognizerResult(\n",
    "                        entity_type=self.ENTITIES[0],\n",
    "                        start=match.start(),\n",
    "                        end=match.end(),\n",
    "                        score=1.0,\n",
    "                    )\n",
    "                )\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def is_valid_rrn(rrn: str) -> bool:\n",
    "        rrn = rrn.replace(\"-\", \"\")\n",
    "        if not rrn.isdigit() or len(rrn) != 13:\n",
    "            return False\n",
    "        check_sum = sum(int(rrn[i]) * ((i % 8) + 2) for i in range(12))\n",
    "        return (11 - (check_sum % 11)) % 10 == int(rrn[12])\n",
    "\n",
    "\n",
    "class KoreanPhoneNumberRecognizer(PatternRecognizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        patterns = [\n",
    "            Pattern(\n",
    "                name=\"Korean Phone Number\",\n",
    "                regex=r\"\\b(?:010|02|0[3-9][0-9])[ -]?(\\d{3,4})[ -]?(\\d{4})\\b\",\n",
    "                score=0.85,\n",
    "            )\n",
    "        ]\n",
    "        super().__init__(\n",
    "            supported_entity=\"KR_PHONE_NUMBER\",\n",
    "            patterns=patterns,\n",
    "            context={\"연락처\", \"휴대폰\", \"전화번호\"},\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "\n",
    "class KoreanBankAccountRecognizer(PatternRecognizer):\n",
    "    def __init__(self, **kwargs):\n",
    "        patterns = [\n",
    "            Pattern(\n",
    "                name=\"Korean Bank Account\",\n",
    "                regex=r\"\\b(?:\\d{2,6}[-]?\\d{2,6}[-]?\\d{3,6}|\\d{10,14})\\b\",\n",
    "                score=0.6,\n",
    "            )\n",
    "        ]\n",
    "        super().__init__(\n",
    "            supported_entity=\"KR_BANK_ACCOUNT_NUMBER\",\n",
    "            patterns=patterns,\n",
    "            context={\"계좌번호\", \"계좌\", \"은행\"},\n",
    "            **kwargs,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0fcac9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KoreanPIIProcessor:\n",
    "    \"\"\"A class to handle PII analysis and anonymization for Korean text.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, hf_model_name: str = \"Leo97/KoELECTRA-small-v3-modu-ner\"\n",
    "    ):  # taeminlee/gliner_ko KPF/KPF-bert-ner Leo97/KoELECTRA-small-v3-modu-ner\n",
    "        \"\"\"\n",
    "        Initializes the processor by setting up the Presidio Analyzer and Anonymizer.\n",
    "\n",
    "        Args:\n",
    "            hf_model_name (str): The Hugging Face model to use for NER.\n",
    "        \"\"\"\n",
    "        self.lang_code = \"ko\"\n",
    "        self.analyzer = self._setup_analyzer(hf_model_name)\n",
    "        self.anonymizer = AnonymizerEngine()\n",
    "        self.faker = Faker(\"ko_KR\")  # For generating fake Korean data\n",
    "        print(\"Korean PII Processor initialized successfully.\")\n",
    "\n",
    "    def _setup_analyzer(self, hf_model_name: str) -> AnalyzerEngine:\n",
    "        \"\"\"Configures and returns a Presidio AnalyzerEngine.\"\"\"\n",
    "        try:\n",
    "            # Using TransformersNlpEngine for state-of-the-art Korean NER\n",
    "            ner_model_configuration = NerModelConfiguration(\n",
    "                # model_to_presidio_entity_mapping=mapping,\n",
    "                alignment_mode=\"expand\",  # \"strict\", \"contract\", \"expand\"\n",
    "                aggregation_strategy=\"max\",  # \"simple\", \"first\", \"average\", \"max\"\n",
    "                # labels_to_ignore = labels_to_ignore\n",
    "            )\n",
    "\n",
    "            nlp_engine = TransformersNlpEngine(\n",
    "                models=[\n",
    "                    {\n",
    "                        \"model_name\": {\"spacy\": \"ko_core_news_sm\", \"transformers\": hf_model_name},\n",
    "                        \"lang_code\": self.lang_code,\n",
    "                    }\n",
    "                ],\n",
    "                ner_model_configuration=ner_model_configuration,\n",
    "            )\n",
    "            print(f\"Successfully loaded Hugging Face NER model: {hf_model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading Hugging Face model '{hf_model_name}': {e}\")\n",
    "            print(\"Falling back to a rule-based only analyzer.\")\n",
    "            nlp_engine = NlpEngine()  # Basic NLP engine without a model\n",
    "\n",
    "        self.nlp_engine = nlp_engine\n",
    "        # Register all custom recognizers\n",
    "        registry = RecognizerRegistry(supported_languages=[self.lang_code])\n",
    "        registry.add_recognizer(KoreanRRNRecognizer(supported_language=self.lang_code))\n",
    "        registry.add_recognizer(KoreanPhoneNumberRecognizer(supported_language=self.lang_code))\n",
    "        registry.add_recognizer(KoreanBankAccountRecognizer(supported_language=self.lang_code))\n",
    "\n",
    "        # Add a recognizer for emails, which is language-agnostic\n",
    "        from presidio_analyzer.predefined_recognizers import EmailRecognizer\n",
    "\n",
    "        registry.add_recognizer(EmailRecognizer())\n",
    "\n",
    "        from presidio_analyzer.predefined_recognizers import (\n",
    "            GLiNERRecognizer,\n",
    "            TransformersRecognizer,\n",
    "        )\n",
    "\n",
    "        gliner_recognizer = GLiNERRecognizer(\n",
    "            supported_language=\"ko\",\n",
    "            model_name=\"taeminlee/gliner_ko\",\n",
    "        )\n",
    "        registry.add_recognizer(gliner_recognizer)\n",
    "\n",
    "        # registry.add_recognizer(TransformersRecognizer(supported_language=\"ko\"))\n",
    "\n",
    "        return AnalyzerEngine(\n",
    "            nlp_engine=nlp_engine,\n",
    "            registry=registry,\n",
    "            supported_languages=[self.lang_code],\n",
    "            # context_aware_enhancer=LemmaContextAwareEnhancer(\n",
    "            #     context_similarity_factor=0.45, min_score_with_context_similarity=0.4\n",
    "            # ),\n",
    "        )\n",
    "\n",
    "    def analyze(self, text: str) -> List[RecognizerResult]:\n",
    "        \"\"\"Analyzes text to find PII.\"\"\"\n",
    "        return self.analyzer.analyze(text=text, language=self.lang_code)\n",
    "\n",
    "    def anonymize(self, text: str, operators: Optional[dict] = None) -> str:\n",
    "        \"\"\"\n",
    "        Analyzes and then anonymizes PII in the text using specified operators.\n",
    "\n",
    "        Args:\n",
    "            text (str): The text to anonymize.\n",
    "            operators (dict, optional): A dictionary defining anonymization strategies\n",
    "                                        for different PII entities.\n",
    "\n",
    "        Returns:\n",
    "            str: The anonymized text.\n",
    "        \"\"\"\n",
    "        if operators is None:\n",
    "            # Default behavior: replace with entity type (e.g., <KR_PHONE_NUMBER>)\n",
    "            operators = {\"DEFAULT\": OperatorConfig(\"replace\")}\n",
    "\n",
    "        analyzer_results = self.analyze(text)\n",
    "        anonymized_result = self.anonymizer.anonymize(\n",
    "            text=text, analyzer_results=analyzer_results, operators=operators\n",
    "        )\n",
    "        return anonymized_result.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96bd18ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded Hugging Face NER model: Leo97/KoELECTRA-small-v3-modu-ner\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e410192908e4a4a8c615452c1e0fc23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shin-younghoon/Desktop/presido/.venv/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:561: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korean PII Processor initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "pii_processor = KoreanPIIProcessor()\n",
    "\n",
    "test_text = (\n",
    "    \"안녕하세요, 제 이름은 홍길동이고, 저의 주민등록번호는 880101-1234567 입니다. \"\n",
    "    \"유효하지 않은 주민번호 991231-1111111도 있습니다. \"\n",
    "    \"제 이메일은 g.hong@example.com이고, 연락처는 010-9876-5432 입니다. \"\n",
    "    \"다른 전화번호는 02 1234 5678 입니다. \"\n",
    "    \"주거래 은행 계좌번호는 110-234-567890 입니다.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f846afff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('안녕하세요, 제 이름은 홍길동이고, 저의 주민등록번호는 880101-1234567 입니다. 유효하지 않은 주민번호 '\n",
      " '991231-1111111도 있습니다. 제 이메일은 g.hong@example.com이고, 연락처는 010-9876-5432 입니다. '\n",
      " '다른 전화번호는 02 1234 5678 입니다. 주거래 은행 계좌번호는 110-234-567890 입니다.')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pprint import pprint\n",
    "\n",
    "pprint(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f54462a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('안녕하세요, 제 이름은 <PERSON>, 저의 주민등록번호는 <ID> 입니다. 유효하지 않은 주민번호 <PERSON> 있습니다. 제 '\n",
      " '이메일은 <EMAIL>, 연락처는 <KR_PHONE_NUMBER> 입니다. 다른 전화번호는 <KR_PHONE_NUMBER> 입니다. '\n",
      " '주거래 은행 계좌번호는 <KR_BANK_ACCOUNT_NUMBER> 입니다.')\n"
     ]
    }
   ],
   "source": [
    "pprint(pii_processor.anonymize(test_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1860648",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "presido",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
